{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Walkthroughs and Exercises for *Fundamentals of Statistics with Python*\"\n",
        "author: \"Dr. Chester Ismay\"\n",
        "format: html\n",
        "engine: knitr\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "import pandas as pd\n",
        "\n",
        "# Display all columns\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1\n",
        "\n",
        "## Walkthrough 1.1: Getting Started\n",
        "\n",
        "### Setting Up the Python Environment\n",
        "\n",
        "If you haven't already installed Python, Jupyter, and the necessary packages, there are instructions on the course repo in the README to do so [here](https://github.com/ismayc/oreilly-fundamentals-of-statistics-with-python/blob/main/README.md). \n",
        "\n",
        "If you aren't able to do this on your machine, you may want to check out [Google Colab](https://colab.research.google.com/). It's a free service that allows you to run Jupyter notebooks in the cloud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Importing libraries/modules and aliasing them as needed\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploring a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load in the dataset\n",
        "data_dev_survey = pd.read_csv(\"data_dev_survey.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display information about the DataFrame\n",
        "data_dev_survey.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performing basic statistical functions using NumPy, Pandas, and SciPy.\n",
        "\n",
        "#### Using NumPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the mean of the years_code_pro column\n",
        "np.mean(data_dev_survey['years_code_pro'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the median of the work_exp column\n",
        "np.median(data_dev_survey['work_exp'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the standard deviation of the converted_comp_yearly column\n",
        "np.std(data_dev_survey['converted_comp_yearly'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display summary statistics on the numeric columns of the DataFrame\n",
        "data_dev_survey.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using SciPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Separate the data into two groups\n",
        "using_ai = data_dev_survey[data_dev_survey['plans_to_use_ai'] == 'Using']['converted_comp_yearly']\n",
        "plan_to_use_ai = data_dev_survey[data_dev_survey['plans_to_use_ai'] == 'Plan to use']['converted_comp_yearly']\n",
        "\n",
        "# Perform the t-test\n",
        "t_stat, p_value = stats.ttest_ind(using_ai, plan_to_use_ai, equal_var=False)\n",
        "\n",
        "# Print the results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1.1: Getting Started\n",
        "\n",
        "### Setting Up the Python Environment\n",
        "\n",
        "If you ran the `# Importing libraries and aliasing them` code above, you should \n",
        "be good to proceed here. If not, scroll up and run it.\n",
        "\n",
        "### Exploring a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load in the coffee_quality dataset\n",
        "coffee_quality = pd.read_csv(\"coffee_quality.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display information about the DataFrame\n",
        "coffee_quality.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performing basic statistical functions using NumPy, Pandas, and SciPy.\n",
        "\n",
        "#### Using NumPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the mean of the aroma column\n",
        "np.mean(coffee_quality['aroma'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the median of the total_cup_points column\n",
        "np.median(coffee_quality['total_cup_points'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the standard deviation of the moisture_percentage column\n",
        "np.std(coffee_quality['moisture_percentage'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display summary statistics on the numeric columns of the DataFrame\n",
        "coffee_quality.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using SciPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Focus on only Asia and North America entries\n",
        "\n",
        "# Separate the data into two groups\n",
        "asian = coffee_quality[coffee_quality['continent_of_origin'] == 'Asia']['total_cup_points']\n",
        "north_american = coffee_quality[coffee_quality['continent_of_origin'] == 'North America']['total_cup_points']\n",
        "\n",
        "# Perform the t-test\n",
        "t_stat, p_value = stats.ttest_ind(asian, north_american, equal_var=False)\n",
        "\n",
        "# Print the results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Walkthrough 1.2: Data Summarizing\n",
        "\n",
        "### Compute and interpret measures of central tendency "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate center statistics for years_code_pro\n",
        "data_dev_survey['years_code_pro'].mean()\n",
        "data_dev_survey['years_code_pro'].median()\n",
        "data_dev_survey['years_code_pro'].mode()\n",
        "\n",
        "# To extract just the value for mode\n",
        "data_dev_survey['years_code_pro'].mode()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute and interpret measures of variation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate spread statistics for years_code_pro\n",
        "range_years_code_pro = data_dev_survey['years_code_pro'].max() - data_dev_survey['years_code_pro'].min()\n",
        "range_years_code_pro\n",
        "\n",
        "data_dev_survey['years_code_pro'].var(ddof=1)\n",
        "data_dev_survey['years_code_pro'].std(ddof=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the five-number summary for 'years_code_pro'\n",
        "min_years_code_pro = data_dev_survey['years_code_pro'].min()\n",
        "q1_years_code_pro = data_dev_survey['years_code_pro'].quantile(0.25)\n",
        "median_years_code_pro = data_dev_survey['years_code_pro'].median()\n",
        "q3_years_code_pro = data_dev_survey['years_code_pro'].quantile(0.75)\n",
        "max_years_code_pro = data_dev_survey['years_code_pro'].max()\n",
        "\n",
        "# Print them out\n",
        "print(f\"Five-Number Summary for years_code_pro:\")\n",
        "print(f\"Minimum: {min_years_code_pro}\")\n",
        "print(f\"First Quartile (Q1): {q1_years_code_pro}\")\n",
        "print(f\"Median (Q2): {median_years_code_pro}\")\n",
        "print(f\"Third Quartile (Q3): {q3_years_code_pro}\")\n",
        "print(f\"Maximum: {max_years_code_pro}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1.2: Data Summarizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate center statistics for 'aroma'\n",
        "mean_aroma = coffee_quality['aroma'].mean()\n",
        "median_aroma = coffee_quality['aroma'].median()\n",
        "mode_aroma = coffee_quality['aroma'].mode()[0]\n",
        "\n",
        "print(f\"Mean of aroma: {mean_aroma}\")\n",
        "print(f\"Median of aroma: {median_aroma}\")\n",
        "print(f\"Mode of aroma: {mode_aroma}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate spread statistics for 'aroma'\n",
        "range_aroma = coffee_quality['aroma'].max() - coffee_quality['aroma'].min()\n",
        "variance_aroma = coffee_quality['aroma'].var(ddof=1)  # Sample variance\n",
        "std_dev_aroma = coffee_quality['aroma'].std(ddof=1)  # Sample standard deviation\n",
        "\n",
        "print(f\"Range of aroma: {range_aroma}\")\n",
        "print(f\"Variance of aroma: {variance_aroma}\")\n",
        "print(f\"Standard Deviation of aroma: {std_dev_aroma}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the five-number summary for 'aroma'\n",
        "min_aroma = coffee_quality['aroma'].min()\n",
        "q1_aroma = coffee_quality['aroma'].quantile(0.25)\n",
        "median_aroma = coffee_quality['aroma'].median()\n",
        "q3_aroma = coffee_quality['aroma'].quantile(0.75)\n",
        "max_aroma = coffee_quality['aroma'].max()\n",
        "\n",
        "print(f\"Five-Number Summary for aroma:\")\n",
        "print(f\"Minimum: {min_aroma}\")\n",
        "print(f\"First Quartile (Q1): {q1_aroma}\")\n",
        "print(f\"Median (Q2): {median_aroma}\")\n",
        "print(f\"Third Quartile (Q3): {q3_aroma}\")\n",
        "print(f\"Maximum: {max_aroma}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Walkthrough 1.3: Cleaning and Preparing Data with Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Histogram for years_code_pro\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data_dev_survey['years_code_pro'])\n",
        "plt.title('Histogram of Years of Coding Experience')\n",
        "plt.xlabel('Years of Coding Experience')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Box plot for work_exp\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(y=data_dev_survey['work_exp'])\n",
        "plt.title('Box Plot of Work Experience')\n",
        "plt.ylabel('Work Experience (years)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scatter plot for years_code_pro vs. converted_comp_yearly\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=data_dev_survey['years_code_pro'], y=data_dev_survey['converted_comp_yearly'])\n",
        "plt.title('Scatter Plot of Years of Coding Experience vs. Yearly Compensation')\n",
        "plt.xlabel('Years of Coding Experience')\n",
        "plt.ylabel('Yearly Compensation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1.3: Cleaning and Preparing Data with Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Histogram for acidity\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(coffee_quality['acidity'])\n",
        "plt.title('Histogram of Coffee Acidity')\n",
        "plt.xlabel('Acidity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Box plot for body\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(y=coffee_quality['body'])\n",
        "plt.title('Box Plot of Coffee Body')\n",
        "plt.ylabel('Body')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scatter plot for body vs. acidity\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=coffee_quality['body'], y=coffee_quality['acidity'])\n",
        "plt.title('Scatter Plot of Body vs. Acidity')\n",
        "plt.xlabel('Body')\n",
        "plt.ylabel('Acidity')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Walkthrough 1.4: Sampling Distribution Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Think of our data as a population to draw from\n",
        "population = data_dev_survey['converted_comp_yearly'].dropna().values\n",
        "\n",
        "# Generate a large sample from the 'converted_comp_yearly' column\n",
        "# Parameters\n",
        "sample_size = 50\n",
        "num_samples = 1000\n",
        "\n",
        "# Set a seed to make code reproducible\n",
        "np.random.seed(2024)\n",
        "\n",
        "# Simulate sampling distribution of the mean\n",
        "sample_means = []\n",
        "for _ in range(num_samples):\n",
        "    sample = np.random.choice(population, sample_size)\n",
        "    sample_means.append(np.mean(sample))\n",
        "\n",
        "# Plot the sampling distribution of the sample means\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(sample_means, bins=30, edgecolor='k', alpha=0.7)\n",
        "plt.title('Sampling Distribution of the Mean (Sample Size = 50)')\n",
        "plt.xlabel('Sample Mean')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1.4: Sampling Distribution Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Think of our aroma data as a population to draw from\n",
        "population = coffee_quality['aroma'].dropna().values\n",
        "\n",
        "# Parameters\n",
        "sample_size = 50\n",
        "num_samples = 1000\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(2024)\n",
        "\n",
        "# Simulate sampling distribution of the mean\n",
        "sample_means = []\n",
        "for _ in range(num_samples):\n",
        "    sample = np.random.choice(population, sample_size)\n",
        "    sample_means.append(np.mean(sample))\n",
        "\n",
        "# Plot the sampling distribution of the sample means\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(sample_means, bins=30, edgecolor='k', alpha=0.7)\n",
        "plt.title('Sampling Distribution of the Mean (Sample Size = 50)')\n",
        "plt.xlabel('Sample Mean')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 2\n",
        "\n",
        "## Walkthrough 2.1: Advanced Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select only numeric columns\n",
        "numeric_columns = data_dev_survey.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculate the correlation matrix for numeric columns\n",
        "correlation_matrix = numeric_columns.corr()\n",
        "\n",
        "# Heatmap for correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Heatmap of Correlation Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pair plot for selected variables 'years_code_pro', 'work_exp', 'converted_comp_yearly'\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.pairplot(data_dev_survey, vars=['years_code_pro', 'work_exp', 'converted_comp_yearly'])\n",
        "plt.suptitle('Pair Plot of Selected Variables', y=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Time series plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "data_dev_survey['survey_completion_date'] = pd.to_datetime(data_dev_survey['survey_completion_date'])\n",
        "\n",
        "# Count the number of surveys completed each day\n",
        "daily_counts = data_dev_survey['survey_completion_date'].value_counts().sort_index()\n",
        "\n",
        "# Plot the counts as a line chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(daily_counts.index, daily_counts.values, marker='o')\n",
        "plt.title('Number of Surveys Completed Each Day in May 2023')\n",
        "plt.xlabel('Day Completed')\n",
        "plt.ylabel('Number of Surveys')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2.1: Advanced Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select only numeric columns\n",
        "numeric_columns = coffee_quality.select_dtypes(include=[np.number])\n",
        "\n",
        "# Drop the 'clean_cup' and 'sweetness' columns since they are always 10\n",
        "numeric_columns = numeric_columns.drop(columns=['clean_cup', 'sweetness'], errors='ignore')\n",
        "\n",
        "# Calculate the correlation matrix for numeric columns\n",
        "correlation_matrix = numeric_columns.corr()\n",
        "\n",
        "# Heatmap for correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Heatmap of Correlation Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pair plot for selected variables ['aroma', 'acidity', 'body']\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.pairplot(coffee_quality, vars=['aroma', 'acidity', 'body'])\n",
        "plt.suptitle('Pair Plot of Selected Variables', y=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- Need to introduce them to dt.to_period() -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot the mean total_cup_points for each grading_date\n",
        "\n",
        "# Convert to datetime\n",
        "coffee_quality['grading_date'] = pd.to_datetime(coffee_quality['grading_date'])\n",
        "\n",
        "# Extract the month and year from the grading_date column\n",
        "coffee_quality['month'] = coffee_quality['grading_date'].dt.to_period('M')\n",
        "\n",
        "# Aggregate the mean total_cup_points by month\n",
        "monthly_mean = coffee_quality.groupby('month')['total_cup_points'].mean()\n",
        "\n",
        "# Plot the mean total_cup_points by month as a line chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_mean.index.astype(str), monthly_mean.values, marker='o')\n",
        "plt.title('Mean Total Cup Points by Month Graded')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Mean Total Cup Points')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Walkthrough 2.2: EDA\n",
        "\n",
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for missing values\n",
        "data_dev_survey.isnull().sum()\n",
        "\n",
        "# Remove duplicates if any\n",
        "data_dev_survey = data_dev_survey.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Histogram for years_code_pro\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data_dev_survey['years_code_pro'])\n",
        "plt.title('Histogram of Years of Coding Experience')\n",
        "plt.xlabel('Years of Coding Experience')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Box plot for work_exp\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(y=data_dev_survey['work_exp'])\n",
        "plt.title('Box Plot of Work Experience')\n",
        "plt.ylabel('Work Experience (years)')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for years_code_pro vs. converted_comp_yearly\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=data_dev_survey['years_code'], y=data_dev_survey['converted_comp_yearly'])\n",
        "plt.title('Scatter Plot of Years of Coding Experience vs. Yearly Compensation')\n",
        "plt.xlabel('Years of Coding Experience')\n",
        "plt.ylabel('Yearly Compensation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate summary statistics\n",
        "data_dev_survey.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2.2: EDA\n",
        "\n",
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for missing values\n",
        "coffee_quality.isnull().sum()\n",
        "\n",
        "# Remove duplicates if any\n",
        "coffee_quality = coffee_quality.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Histogram for aroma\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(coffee_quality['aroma'])\n",
        "plt.title('Histogram of Coffee Aroma')\n",
        "plt.xlabel('Aroma')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Box plot for acidity\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(y=coffee_quality['acidity'])\n",
        "plt.title('Box Plot of Coffee Acidity')\n",
        "plt.ylabel('Acidity')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for aroma vs. total_cup_points\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=coffee_quality['aroma'], y=coffee_quality['total_cup_points'])\n",
        "plt.title('Scatter Plot of Aroma vs. Total Cup Points')\n",
        "plt.xlabel('Aroma')\n",
        "plt.ylabel('Total Cup Points')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate summary statistics\n",
        "coffee_quality.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Walkthrough 2.3: Data Preprocessing\n",
        "\n",
        "### Inspect the Data after Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display basic information about the dataset\n",
        "data_dev_survey.info()\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "data_dev_survey.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for missing values\n",
        "data_dev_survey.isnull().sum()\n",
        "\n",
        "# Make a copy of the dataset for imputation\n",
        "data_dev_survey_imputed = data_dev_survey.copy()\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_columns = data_dev_survey_imputed.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Fill missing values in numeric columns with the median\n",
        "data_dev_survey_imputed[numeric_columns] = data_dev_survey_imputed[numeric_columns].fillna(data_dev_survey_imputed[numeric_columns].median())\n",
        "\n",
        "# Display the first few rows of the imputed dataset\n",
        "data_dev_survey_imputed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select only numeric columns\n",
        "numeric_data = data_dev_survey_imputed.select_dtypes(include=[np.number])\n",
        "\n",
        "# Identify outliers using IQR\n",
        "Q1 = numeric_data.quantile(0.25)\n",
        "Q3 = numeric_data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = numeric_data[(numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))]\n",
        "print(outliers)\n",
        "\n",
        "# Remove outliers\n",
        "data_dev_survey_imputed = data_dev_survey_imputed[~((numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))).any(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Normalization on converted_comp_yearly\n",
        "scaler = MinMaxScaler()\n",
        "data_dev_survey_imputed[['converted_comp_yearly']] = scaler.fit_transform(data_dev_survey_imputed[['converted_comp_yearly']])\n",
        "\n",
        "# Standardization on work_exp\n",
        "scaler = StandardScaler()\n",
        "data_dev_survey_imputed[['work_exp']] = scaler.fit_transform(data_dev_survey_imputed[['work_exp']])\n",
        "\n",
        "# Encoding categorical variables (country)\n",
        "data_dev_survey_imputed = pd.get_dummies(data_dev_survey_imputed, columns=['country'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Visualizations on Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Histogram for years_code_pro\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data_dev_survey_imputed['years_code_pro'])\n",
        "plt.title('Histogram of Years of Coding Experience')\n",
        "plt.xlabel('Years of Coding Experience')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Box plot for work_exp\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(y=data_dev_survey_imputed['work_exp'])\n",
        "plt.title('Box Plot of Work Experience')\n",
        "plt.ylabel('Work Experience (years)')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for years_code_pro vs. converted_comp_yearly\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=data_dev_survey_imputed['years_code_pro'], y=data_dev_survey_imputed['converted_comp_yearly'])\n",
        "plt.title('Scatter Plot of Years of Coding Experience vs. Yearly Compensation')\n",
        "plt.xlabel('Years of Coding Experience')\n",
        "plt.ylabel('Yearly Compensation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2.3: Data Preprocessing\n",
        "\n",
        "### Inspect the Data after Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display basic information about the dataset\n",
        "coffee_quality.info()\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "coffee_quality.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check for missing values\n",
        "coffee_quality.isnull().sum()\n",
        "\n",
        "# Make a copy of the dataset for imputation\n",
        "coffee_quality_imputed = coffee_quality.copy()\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_columns = coffee_quality_imputed.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Fill missing values in numeric columns with the median\n",
        "coffee_quality_imputed[numeric_columns] = coffee_quality_imputed[numeric_columns].fillna(coffee_quality_imputed[numeric_columns].median())\n",
        "\n",
        "# Display the first few rows of the imputed dataset\n",
        "coffee_quality_imputed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handle Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select only numeric columns\n",
        "numeric_data = coffee_quality_imputed.select_dtypes(include=[np.number])\n",
        "\n",
        "# Identify outliers using IQR\n",
        "Q1 = numeric_data.quantile(0.25)\n",
        "Q3 = numeric_data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = numeric_data[(numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))]\n",
        "print(outliers)\n",
        "\n",
        "# Remove outliers\n",
        "coffee_quality_imputed = coffee_quality_imputed[~((numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))).any(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Normalization on total_cup_points\n",
        "scaler = MinMaxScaler()\n",
        "coffee_quality_imputed[['total_cup_points']] = scaler.fit_transform(coffee_quality_imputed[['total_cup_points']])\n",
        "# Can use this instead\n",
        "coffee_quality_imputed.loc[:, ['total_cup_points']] = scaler.fit_transform(coffee_quality_imputed[['total_cup_points']])\n",
        "\n",
        "# Standardization on acidity\n",
        "scaler = StandardScaler()\n",
        "coffee_quality_imputed[['acidity']] = scaler.fit_transform(coffee_quality_imputed[['acidity']])\n",
        "# Or this\n",
        "coffee_quality_imputed.loc[:, ['acidity']] = scaler.fit_transform(coffee_quality_imputed[['acidity']])\n",
        "\n",
        "# Encoding categorical variables (country and continent of origin)\n",
        "coffee_quality_imputed = pd.get_dummies(coffee_quality_imputed, columns=['country_of_origin', 'continent_of_origin'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Visualizations on Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Histogram for aroma\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(coffee_quality_imputed['aroma'])\n",
        "plt.title('Histogram of Coffee Aroma')\n",
        "plt.xlabel('Aroma')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Box plot for acidity\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(y=coffee_quality_imputed['acidity'])\n",
        "plt.title('Box Plot of Coffee Acidity')\n",
        "plt.ylabel('Acidity')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for aroma vs. total_cup_points\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=coffee_quality_imputed['aroma'], y=coffee_quality_imputed['total_cup_points'])\n",
        "plt.title('Scatter Plot of Aroma vs. Total Cup Points')\n",
        "plt.xlabel('Aroma')\n",
        "plt.ylabel('Total Cup Points')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Walkthrough 2.4: Correlations\n",
        "\n",
        "### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select only numeric columns\n",
        "numeric_columns = data_dev_survey.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numeric_columns.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "correlation_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Heatmap for correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Heatmap of Correlation Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Scatter Plots for Meaningful Correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scatter plot for years_code_pro vs. years_code\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=data_dev_survey['years_code_pro'], y=data_dev_survey['years_code'])\n",
        "plt.title('Scatter Plot of Years of Professional Coding Experience vs. Years of Professional Coding Experience')\n",
        "plt.xlabel('Years of Professional Coding Experience')\n",
        "plt.ylabel('Years of Coding Experience')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for work_exp vs. years_code_pro\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=data_dev_survey['work_exp'], y=data_dev_survey['years_code_pro'])\n",
        "plt.title('Scatter Plot of Work Experience vs. Years of Professional Coding Experience')\n",
        "plt.xlabel('Work Experience')\n",
        "plt.ylabel('Years of Professional Coding Experience')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2.4: Correlations\n",
        "\n",
        "### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select only numeric columns\n",
        "numeric_columns = coffee_quality.select_dtypes(include=[np.number])\n",
        "\n",
        "# Remove clean_cup and sweetness too\n",
        "numeric_columns = numeric_columns.drop(columns=['clean_cup', 'sweetness'], errors='ignore')\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numeric_columns.corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "correlation_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Heatmap for correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Heatmap of Correlation Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Scatter Plots for Meaningful Correlations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scatter plot for flavor vs. total_cup_points\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=coffee_quality['flavor'], y=coffee_quality['total_cup_points'])\n",
        "plt.title('Scatter Plot of Flavor vs. Total Cup Points')\n",
        "plt.xlabel('Flavor')\n",
        "plt.ylabel('Total Cup Points')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for overall vs. total_cup_points\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.scatterplot(x=coffee_quality['overall'], y=coffee_quality['total_cup_points'])\n",
        "plt.title('Scatter Plot of Overall vs. Total Cup Points')\n",
        "plt.xlabel('Overall')\n",
        "plt.ylabel('Total Cup Points')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3\n",
        "\n",
        "## Walkthrough 3.1: Simulating Distributions\n",
        "\n",
        "### Simulating Binomial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate binomial distribution\n",
        "binom_samples = np.random.binomial(n=10, p=0.5, size=10000)\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(binom_samples, kde=False, bins=30)\n",
        "plt.title('Binomial Distribution (n=10, p=0.5)')\n",
        "plt.xlabel('Number of Successes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate normal distribution\n",
        "normal_samples = np.random.normal(loc=0, scale=1, size=10000)\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(normal_samples, kde=True, bins=30)\n",
        "plt.title('Normal Distribution (μ=0, σ=1)')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3.1: \n",
        "\n",
        "### Simulating Poisson"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate Poisson distribution with lambda (lam) parameter 3\n",
        "poisson_samples = np.random.poisson(lam=3, size=10000)\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(poisson_samples, kde=False, bins=30)\n",
        "plt.title('Poisson Distribution (λ=3)')\n",
        "plt.xlabel('Number of Events')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulating Exponential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate exponential distribution with scale parameter 1\n",
        "exponential_samples = np.random.exponential(scale=1, size=10000)\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(exponential_samples, kde=True, bins=30)\n",
        "plt.title('Exponential Distribution (λ=1)')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Walkthrough 3.2: t-tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One-sample t-test checking for evidence that mu compensation > 85000\n",
        "salary_mean = 85000\n",
        "t_stat, p_value = stats.ttest_1samp(\n",
        "  data_dev_survey['converted_comp_yearly'].dropna(), \n",
        "  popmean = salary_mean,\n",
        "  alternative='greater')\n",
        "\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Two-sample t-test comparing compensation across plans_to_use_ai groups\n",
        "# Checking for a difference\n",
        "using = data_dev_survey[data_dev_survey['plans_to_use_ai'] == 'Using']['converted_comp_yearly'].dropna()\n",
        "plan_to_use = data_dev_survey[data_dev_survey['plans_to_use_ai'] == 'Plan to use']['converted_comp_yearly'].dropna()\n",
        "t_stat, p_value = stats.ttest_ind(using, plan_to_use, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3.2: t-tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One-sample t-test checking for evidence that mu flavor < 7.8\n",
        "flavor_mean = 7.8\n",
        "t_stat, p_value = stats.ttest_1samp(\n",
        "  coffee_quality['flavor'].dropna(), \n",
        "  flavor_mean,\n",
        "  alternative='less')\n",
        "\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Two-sample t-test checking for difference in Columbia and Brazil total_cup_points\n",
        "colombia = coffee_quality[coffee_quality['country_of_origin'] == 'Colombia']['total_cup_points'].dropna()\n",
        "brazil = coffee_quality[coffee_quality['country_of_origin'] == 'Brazil']['total_cup_points'].dropna()\n",
        "t_stat, p_value = stats.ttest_ind(colombia, brazil, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Walkthrough 3.3: Comparative Tests\n",
        "\n",
        "<!-- Also show EDA first to make guesses as to statistical significance -->\n",
        "\n",
        "<!-- Look up *groups again -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform one-way ANOVA comparing compensation across different levels of\n",
        "# remote_work\n",
        "groups = data_dev_survey.groupby('remote_work')['converted_comp_yearly'].apply(list)\n",
        "f_stat, p_value = stats.f_oneway(*groups)\n",
        "\n",
        "print(f\"F-statistic: {f_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a contingency table of employment and remote_work\n",
        "contingency_table = pd.crosstab(data_dev_survey['employment'], \n",
        "                                data_dev_survey['remote_work'])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2}\")\n",
        "print(f\"P-value: {p}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3.3: Comparative Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform one-way ANOVA comparing total_cup_points across country_of_origin\n",
        "groups = coffee_quality.groupby('country_of_origin')['total_cup_points'].apply(list)\n",
        "f_stat, p_value = stats.f_oneway(*groups)\n",
        "\n",
        "print(f\"F-statistic: {f_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform a chi-square test of independence for processing_method versus\n",
        "# continent_of_origin\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(coffee_quality['processing_method'], coffee_quality['continent_of_origin'])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2}\")\n",
        "print(f\"P-value: {p}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Walkthrough 3.4: Non-Parametric Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform Mann-Whitney U Test comparing compensation for Remote and In-person\n",
        "group1 = data_dev_survey[data_dev_survey['remote_work'] == 'Remote']['converted_comp_yearly'].dropna()\n",
        "group2 = data_dev_survey[data_dev_survey['remote_work'] == 'In-person']['converted_comp_yearly'].dropna()\n",
        "stat, p_value = stats.mannwhitneyu(group1, group2)\n",
        "\n",
        "print(f\"Mann-Whitney U Statistic: {stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform Kruskal-Wallis H Test comparing compensation across countries\n",
        "groups = data_dev_survey.groupby('country')['converted_comp_yearly'].apply(list)\n",
        "stat, p_value = stats.kruskal(*groups)\n",
        "\n",
        "print(f\"Kruskal-Wallis H Statistic: {stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3.4: Non-Parametric Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform Mann-Whitney U Test comparing total_cup_points for Guatemala\n",
        "# and Honduras\n",
        "group1 = coffee_quality[coffee_quality['country_of_origin'] == 'Guatemala']['total_cup_points'].dropna()\n",
        "group2 = coffee_quality[coffee_quality['country_of_origin'] == 'Honduras']['total_cup_points'].dropna()\n",
        "stat, p_value = stats.mannwhitneyu(group1, group2)\n",
        "\n",
        "print(f\"Mann-Whitney U Statistic: {stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform Kruskal-Wallis H Test comparing total_cup_points across \n",
        "# continent_of_origin\n",
        "groups = coffee_quality.groupby('continent_of_origin')['total_cup_points'].apply(list)\n",
        "stat, p_value = stats.kruskal(*groups)\n",
        "\n",
        "print(f\"Kruskal-Wallis H Statistic: {stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}